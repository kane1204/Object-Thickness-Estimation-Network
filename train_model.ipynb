{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.thickness_dataset import ThicknessDataset, ToTensor\n",
    "from src.models import U_Net\n",
    "from src.resnet import ResNet\n",
    "from src.trainer import Trainer\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "PATH = \"data\\gen_data_6000_v2\"\n",
    "\n",
    "\n",
    "augs = A.Compose([A.Normalize(mean = (0, 0, 0),\n",
    "                              std  = (1, 1, 1)),\n",
    "                 ToTensorV2()])\n",
    "thickness_dataset = ThicknessDataset(PATH,  transform=augs)\n",
    "\n",
    "# https://kozodoi.me/python/deep%20learning/pytorch/tutorial/2021/03/08/image-mean-std.html Could be worth attempting to normalise the data\n",
    "# https://stackoverflow.com/questions/41428868/image-preprocessing-in-deep-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cam_pos:  [ 1.27875577  1.78995832 -1.17304301]\n",
      "catagory:  airplane,aeroplane,plane\n",
      "model_id:  172764bea108bbcceae5a783c313eb36\n",
      "sample_no:  frame_0\n"
     ]
    }
   ],
   "source": [
    "idx = 0 \n",
    "sample = thickness_dataset[idx]\n",
    "\n",
    "print(\"cam_pos: \", sample['cam_pos'])\n",
    "print(\"catagory: \", sample['catagory'])\n",
    "print(\"model_id: \", sample['model_id'])\n",
    "print(\"sample_no: \", sample['sample_no'])\n",
    "\n",
    "img = sample['img']\n",
    "vis = sample['depth_map'].reshape(128,128)\n",
    "# vis = vis[np.nonzero(vis)]\n",
    "\n",
    "# print(img.shape)\n",
    "# img = img.numpy().transpose(1, 2, 0)\n",
    "\n",
    "# # Displays ehhe\n",
    "# fig = plt.figure(figsize=(8,6))\n",
    "# ax = fig.add_subplot()\n",
    "# ax.imshow(vis)\n",
    "\n",
    "# plt.title(f\"{sample['catagory']}, {sample['model_id']}, {sample['sample_no']}\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52/52 [00:31<00:00,  1.66it/s]\n"
     ]
    }
   ],
   "source": [
    "# Calculate mean and std\n",
    "image_loader = DataLoader(thickness_dataset, \n",
    "                          batch_size  = 128, \n",
    "                          shuffle     = False, \n",
    "                          num_workers = 4,\n",
    "                          pin_memory  = True)\n",
    "\n",
    "# placeholders\n",
    "psum    = torch.tensor([0.0, 0.0, 0.0])\n",
    "psum_sq = torch.tensor([0.0, 0.0, 0.0])\n",
    "\n",
    "# loop through images\n",
    "for inputs in tqdm(image_loader):\n",
    "    psum    += inputs['img'].sum(axis        = [0, 2, 3])\n",
    "    psum_sq += (inputs['img'] ** 2).sum(axis = [0, 2, 3])\n",
    "\n",
    "# pixel count\n",
    "count = len(thickness_dataset) * 128 * 128\n",
    "\n",
    "# mean and std\n",
    "total_mean = psum / count\n",
    "total_var  = (psum_sq / count) - (total_mean ** 2)\n",
    "total_std  = torch.sqrt(total_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "augs = A.Compose([A.Normalize(mean = total_mean,\n",
    "                              std  = total_std),\n",
    "                 ToTensorV2()])\n",
    "thickness_dataset = ThicknessDataset(PATH, transform=augs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cam_pos:  [ 1.27875577  1.78995832 -1.17304301]\n",
      "catagory:  airplane,aeroplane,plane\n",
      "model_id:  172764bea108bbcceae5a783c313eb36\n",
      "sample_no:  frame_0\n",
      "torch.Size([3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "idx = 0 \n",
    "sample = thickness_dataset[idx]\n",
    "\n",
    "print(\"cam_pos: \", sample['cam_pos'])\n",
    "print(\"catagory: \", sample['catagory'])\n",
    "print(\"model_id: \", sample['model_id'])\n",
    "print(\"sample_no: \", sample['sample_no'])\n",
    "\n",
    "img = sample['img']\n",
    "print(img.shape)\n",
    "img = img.numpy().transpose(1, 2, 0)\n",
    "\n",
    "# Displays ehhe\n",
    "# fig = plt.figure(figsize=(8,6))\n",
    "# ax = fig.add_subplot()\n",
    "# ax.imshow(img)\n",
    "# plt.title(f\"{sample['catagory']}, {sample['model_id']}, {sample['sample_no']}\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = 0.7\n",
    "valid_split = 0.1\n",
    "train_size = int(train_split * len(thickness_dataset))\n",
    "valid_size = int(valid_split * len(thickness_dataset))\n",
    "test_size = len(thickness_dataset) - (train_size+valid_size)\n",
    "train_dataset,valid_dataset ,test_dataset = torch.utils.data.random_split(thickness_dataset, [train_size, valid_size, test_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "batch_size = 16 # 16\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Catagory: 02808440, Model: 88d183f319cddb7344221bef0fa3c36b,Sample: frame_0\n",
      "\n",
      "Catagory: 02880940, Model: 899af991203577f019790c8746d79a6f,Sample: frame_0\n",
      "\n",
      "Catagory: 02942699, Model: ce40b134b11e8c822bbc2c380e91dfe2,Sample: frame_1\n",
      "\n",
      "Catagory: 02958343, Model: ba494b33be3a3e0dc1bbb501b1d87871,Sample: frame_1\n",
      "\n",
      "Catagory: 03761084, Model: 46dbba829a57ace8cffd61677456447e,Sample: frame_0\n",
      "\n",
      "Catagory: 03991062, Model: 67bc9bec05f78850f9e08161aea27d2f,Sample: frame_0\n",
      "\n",
      "Catagory: 04225987, Model: 1d527bbed4d12817fa3bb91f4e3cd35f,Sample: frame_0\n",
      "\n",
      "Catagory: 04256520, Model: bc6a3fa659dd7ec0c62ac18334863d36,Sample: frame_1\n",
      "\n",
      "Catagory: 04401088, Model: f400eb5421283e8a102f4912aece242b,Sample: frame_2\n",
      "\n",
      "Catagory: 04460130, Model: 15cc3d9020384e8d6e09a8e31c7575c5,Sample: frame_2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from os import listdir\n",
    "# from os.path import isfile, join\n",
    "# dir = \"data\\human_samples\"\n",
    "# catagories = []\n",
    "# for cat in listdir(dir):\n",
    "#     catagories.append(cat)\n",
    "# create a list of samples from the test dataset wher each of these catagorie ids\n",
    "# are present\n",
    "# test_samples = [[]]*len(catagories)\n",
    "# for i in range(len(catagories)):\n",
    "#     test_samples[i] = [x for x in test_dataset if x['catagory_id'] == catagories[i]]\n",
    "\n",
    "# print the catagory_id, model_id and sample_no of first samples of each catagory\n",
    "# for i in range(len(catagories)):\n",
    "#     print(f\"Catagory: {catagories[i]}, Model: {test_samples[i][0]['model_id']},Sample: {test_samples[i][0]['sample_no']}\")\n",
    "#     print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class berHuLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(berHuLoss, self).__init__()\n",
    "\n",
    "    def forward(self, pred, target, delta=1.0):\n",
    "        assert pred.dim() == target.dim(), \"inconsistent dimensions\"\n",
    "\n",
    "        error = target - pred\n",
    "        abs_error = torch.abs(error)\n",
    "        mask = abs_error < delta\n",
    "        squared_loss = 0.5 * torch.square(error)\n",
    "        linear_loss = delta * (abs_error - 0.5 * delta)\n",
    "        loss = torch.where(mask, squared_loss, linear_loss)\n",
    "        return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91622d60e81744a3ba11468b518b927b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch: 0 | Train Masked Loss: 0.02262 | Train Loss: 0.00562 | Val  Masked Loss: 0.01387 | Val Loss: 0.00179\n",
      "Finished Epoch: 1 | Train Masked Loss: 0.01101 | Train Loss: 0.00156 | Val  Masked Loss: 0.01202 | Val Loss: 0.00145\n",
      "Finished Epoch: 2 | Train Masked Loss: 0.01104 | Train Loss: 0.00152 | Val  Masked Loss: 0.01101 | Val Loss: 0.00144\n",
      "Finished Epoch: 3 | Train Masked Loss: 0.01013 | Train Loss: 0.00140 | Val  Masked Loss: 0.00939 | Val Loss: 0.00128\n",
      "Finished Epoch: 4 | Train Masked Loss: 0.01009 | Train Loss: 0.00138 | Val  Masked Loss: 0.00844 | Val Loss: 0.00116\n",
      "Finished Epoch: 5 | Train Masked Loss: 0.01016 | Train Loss: 0.00138 | Val  Masked Loss: 0.00876 | Val Loss: 0.00121\n",
      "Finished Epoch: 6 | Train Masked Loss: 0.00981 | Train Loss: 0.00132 | Val  Masked Loss: 0.00821 | Val Loss: 0.00115\n",
      "Finished Epoch: 7 | Train Masked Loss: 0.01004 | Train Loss: 0.00136 | Val  Masked Loss: 0.00925 | Val Loss: 0.00130\n",
      "Finished Epoch: 8 | Train Masked Loss: 0.01022 | Train Loss: 0.00137 | Val  Masked Loss: 0.02369 | Val Loss: 0.00353\n",
      "Finished Epoch: 9 | Train Masked Loss: 0.01027 | Train Loss: 0.00140 | Val  Masked Loss: 0.01191 | Val Loss: 0.00168\n",
      "Finished Epoch: 10 | Train Masked Loss: 0.00977 | Train Loss: 0.00130 | Val  Masked Loss: 0.00843 | Val Loss: 0.00121\n",
      "Finished Epoch: 11 | Train Masked Loss: 0.01044 | Train Loss: 0.00140 | Val  Masked Loss: 0.00900 | Val Loss: 0.00125\n",
      "Finished Epoch: 12 | Train Masked Loss: 0.01011 | Train Loss: 0.00135 | Val  Masked Loss: 0.01082 | Val Loss: 0.00160\n",
      "Finished Epoch: 13 | Train Masked Loss: 0.01084 | Train Loss: 0.00142 | Val  Masked Loss: 0.00986 | Val Loss: 0.00136\n",
      "Finished Epoch: 14 | Train Masked Loss: 0.01060 | Train Loss: 0.00138 | Val  Masked Loss: 0.01260 | Val Loss: 0.00174\n",
      "Finished Epoch: 15 | Train Masked Loss: 0.01048 | Train Loss: 0.00138 | Val  Masked Loss: 0.01011 | Val Loss: 0.00138\n",
      "Finished Epoch: 16 | Train Masked Loss: 0.01063 | Train Loss: 0.00139 | Val  Masked Loss: 0.01317 | Val Loss: 0.00182\n",
      "Finished Epoch: 17 | Train Masked Loss: 0.01040 | Train Loss: 0.00136 | Val  Masked Loss: 0.00930 | Val Loss: 0.00130\n",
      "Finished Epoch: 18 | Train Masked Loss: 0.01033 | Train Loss: 0.00135 | Val  Masked Loss: 0.01429 | Val Loss: 0.00197\n",
      "Finished Epoch: 19 | Train Masked Loss: 0.01044 | Train Loss: 0.00136 | Val  Masked Loss: 0.01031 | Val Loss: 0.00142\n",
      "Finished Epoch: 20 | Train Masked Loss: 0.01031 | Train Loss: 0.00134 | Val  Masked Loss: 0.01365 | Val Loss: 0.00160\n",
      "Finished Epoch: 21 | Train Masked Loss: 0.01041 | Train Loss: 0.00136 | Val  Masked Loss: 0.01829 | Val Loss: 0.00245\n",
      "Finished Epoch: 22 | Train Masked Loss: 0.01027 | Train Loss: 0.00134 | Val  Masked Loss: 0.01003 | Val Loss: 0.00144\n",
      "Finished Epoch: 23 | Train Masked Loss: 0.01072 | Train Loss: 0.00139 | Val  Masked Loss: 0.01180 | Val Loss: 0.00162\n",
      "Finished Epoch: 24 | Train Masked Loss: 0.01060 | Train Loss: 0.00138 | Val  Masked Loss: 0.01265 | Val Loss: 0.00171\n",
      "Finished Epoch: 25 | Train Masked Loss: 0.00970 | Train Loss: 0.00126 | Val  Masked Loss: 0.01054 | Val Loss: 0.00147\n",
      "Finished Epoch: 26 | Train Masked Loss: 0.00943 | Train Loss: 0.00124 | Val  Masked Loss: 0.00965 | Val Loss: 0.00135\n",
      "Finished Epoch: 27 | Train Masked Loss: 0.00918 | Train Loss: 0.00120 | Val  Masked Loss: 0.01109 | Val Loss: 0.00153\n",
      "Finished Epoch: 28 | Train Masked Loss: 0.00873 | Train Loss: 0.00115 | Val  Masked Loss: 0.01004 | Val Loss: 0.00138\n",
      "Finished Epoch: 29 | Train Masked Loss: 0.00847 | Train Loss: 0.00112 | Val  Masked Loss: 0.00837 | Val Loss: 0.00106\n",
      "Finished Epoch: 30 | Train Masked Loss: 0.00845 | Train Loss: 0.00112 | Val  Masked Loss: 0.00891 | Val Loss: 0.00107\n",
      "Finished Epoch: 31 | Train Masked Loss: 0.00835 | Train Loss: 0.00111 | Val  Masked Loss: 0.00784 | Val Loss: 0.00102\n",
      "Finished Epoch: 32 | Train Masked Loss: 0.00808 | Train Loss: 0.00107 | Val  Masked Loss: 0.00798 | Val Loss: 0.00102\n",
      "Finished Epoch: 33 | Train Masked Loss: 0.00792 | Train Loss: 0.00105 | Val  Masked Loss: 0.00768 | Val Loss: 0.00100\n",
      "Finished Epoch: 34 | Train Masked Loss: 0.00805 | Train Loss: 0.00106 | Val  Masked Loss: 0.00769 | Val Loss: 0.00097\n",
      "Finished Epoch: 35 | Train Masked Loss: 0.00794 | Train Loss: 0.00106 | Val  Masked Loss: 0.00808 | Val Loss: 0.00099\n",
      "Finished Epoch: 36 | Train Masked Loss: 0.00786 | Train Loss: 0.00104 | Val  Masked Loss: 0.00854 | Val Loss: 0.00104\n",
      "Finished Epoch: 37 | Train Masked Loss: 0.00756 | Train Loss: 0.00101 | Val  Masked Loss: 0.00760 | Val Loss: 0.00096\n",
      "Finished Epoch: 38 | Train Masked Loss: 0.00752 | Train Loss: 0.00100 | Val  Masked Loss: 0.00766 | Val Loss: 0.00096\n",
      "Finished Epoch: 39 | Train Masked Loss: 0.00751 | Train Loss: 0.00100 | Val  Masked Loss: 0.00771 | Val Loss: 0.00095\n",
      "Finished Epoch: 40 | Train Masked Loss: 0.00746 | Train Loss: 0.00100 | Val  Masked Loss: 0.00777 | Val Loss: 0.00098\n",
      "Finished Epoch: 41 | Train Masked Loss: 0.00737 | Train Loss: 0.00099 | Val  Masked Loss: 0.00709 | Val Loss: 0.00094\n",
      "Finished Epoch: 42 | Train Masked Loss: 0.00728 | Train Loss: 0.00098 | Val  Masked Loss: 0.00750 | Val Loss: 0.00095\n",
      "Finished Epoch: 43 | Train Masked Loss: 0.00727 | Train Loss: 0.00097 | Val  Masked Loss: 0.00775 | Val Loss: 0.00097\n",
      "Finished Epoch: 44 | Train Masked Loss: 0.00720 | Train Loss: 0.00096 | Val  Masked Loss: 0.00756 | Val Loss: 0.00099\n",
      "Finished Epoch: 45 | Train Masked Loss: 0.00719 | Train Loss: 0.00096 | Val  Masked Loss: 0.00816 | Val Loss: 0.00103\n",
      "Finished Epoch: 46 | Train Masked Loss: 0.00716 | Train Loss: 0.00096 | Val  Masked Loss: 0.00799 | Val Loss: 0.00098\n",
      "Finished Epoch: 47 | Train Masked Loss: 0.00707 | Train Loss: 0.00094 | Val  Masked Loss: 0.00771 | Val Loss: 0.00098\n",
      "Finished Epoch: 48 | Train Masked Loss: 0.00713 | Train Loss: 0.00095 | Val  Masked Loss: 0.00741 | Val Loss: 0.00095\n",
      "Finished Epoch: 49 | Train Masked Loss: 0.00704 | Train Loss: 0.00094 | Val  Masked Loss: 0.00834 | Val Loss: 0.00107\n",
      "Finished Epoch: 50 | Train Masked Loss: 0.00704 | Train Loss: 0.00094 | Val  Masked Loss: 0.00746 | Val Loss: 0.00098\n",
      "Finished Epoch: 51 | Train Masked Loss: 0.00710 | Train Loss: 0.00094 | Val  Masked Loss: 0.00714 | Val Loss: 0.00095\n",
      "Finished Epoch: 52 | Train Masked Loss: 0.00695 | Train Loss: 0.00092 | Val  Masked Loss: 0.00742 | Val Loss: 0.00097\n",
      "Finished Epoch: 53 | Train Masked Loss: 0.00692 | Train Loss: 0.00092 | Val  Masked Loss: 0.00744 | Val Loss: 0.00099\n",
      "Finished Epoch: 54 | Train Masked Loss: 0.00693 | Train Loss: 0.00092 | Val  Masked Loss: 0.00714 | Val Loss: 0.00090\n",
      "Finished Epoch: 55 | Train Masked Loss: 0.00697 | Train Loss: 0.00092 | Val  Masked Loss: 0.00677 | Val Loss: 0.00090\n",
      "Finished Epoch: 56 | Train Masked Loss: 0.00692 | Train Loss: 0.00092 | Val  Masked Loss: 0.00721 | Val Loss: 0.00095\n",
      "Finished Epoch: 57 | Train Masked Loss: 0.00686 | Train Loss: 0.00091 | Val  Masked Loss: 0.00767 | Val Loss: 0.00096\n",
      "Finished Epoch: 58 | Train Masked Loss: 0.00686 | Train Loss: 0.00091 | Val  Masked Loss: 0.00944 | Val Loss: 0.00127\n",
      "Finished Epoch: 59 | Train Masked Loss: 0.00686 | Train Loss: 0.00091 | Val  Masked Loss: 0.00701 | Val Loss: 0.00092\n",
      "Finished Epoch: 60 | Train Masked Loss: 0.00687 | Train Loss: 0.00091 | Val  Masked Loss: 0.00710 | Val Loss: 0.00092\n",
      "Finished Epoch: 61 | Train Masked Loss: 0.00671 | Train Loss: 0.00089 | Val  Masked Loss: 0.00715 | Val Loss: 0.00096\n",
      "Finished Epoch: 62 | Train Masked Loss: 0.00675 | Train Loss: 0.00090 | Val  Masked Loss: 0.00700 | Val Loss: 0.00090\n",
      "Finished Epoch: 63 | Train Masked Loss: 0.00672 | Train Loss: 0.00089 | Val  Masked Loss: 0.00721 | Val Loss: 0.00096\n",
      "Finished Epoch: 64 | Train Masked Loss: 0.00674 | Train Loss: 0.00090 | Val  Masked Loss: 0.00712 | Val Loss: 0.00096\n",
      "Finished Epoch: 65 | Train Masked Loss: 0.00686 | Train Loss: 0.00091 | Val  Masked Loss: 0.00728 | Val Loss: 0.00098\n",
      "Finished Epoch: 66 | Train Masked Loss: 0.00676 | Train Loss: 0.00090 | Val  Masked Loss: 0.00677 | Val Loss: 0.00088\n",
      "Finished Epoch: 67 | Train Masked Loss: 0.00665 | Train Loss: 0.00088 | Val  Masked Loss: 0.00728 | Val Loss: 0.00099\n",
      "Finished Epoch: 68 | Train Masked Loss: 0.00682 | Train Loss: 0.00091 | Val  Masked Loss: 0.00714 | Val Loss: 0.00096\n",
      "Finished Epoch: 69 | Train Masked Loss: 0.00667 | Train Loss: 0.00089 | Val  Masked Loss: 0.00788 | Val Loss: 0.00107\n",
      "Finished Epoch: 70 | Train Masked Loss: 0.00679 | Train Loss: 0.00090 | Val  Masked Loss: 0.00704 | Val Loss: 0.00092\n",
      "Finished Epoch: 71 | Train Masked Loss: 0.00653 | Train Loss: 0.00087 | Val  Masked Loss: 0.00707 | Val Loss: 0.00090\n",
      "Finished Epoch: 72 | Train Masked Loss: 0.00654 | Train Loss: 0.00087 | Val  Masked Loss: 0.00715 | Val Loss: 0.00095\n",
      "Finished Epoch: 73 | Train Masked Loss: 0.00660 | Train Loss: 0.00088 | Val  Masked Loss: 0.00730 | Val Loss: 0.00091\n",
      "Finished Epoch: 74 | Train Masked Loss: 0.00654 | Train Loss: 0.00087 | Val  Masked Loss: 0.00714 | Val Loss: 0.00097\n",
      "Finished Epoch: 75 | Train Masked Loss: 0.00657 | Train Loss: 0.00088 | Val  Masked Loss: 0.00712 | Val Loss: 0.00090\n",
      "Finished Epoch: 76 | Train Masked Loss: 0.00657 | Train Loss: 0.00087 | Val  Masked Loss: 0.00729 | Val Loss: 0.00098\n",
      "Finished Epoch: 77 | Train Masked Loss: 0.00662 | Train Loss: 0.00088 | Val  Masked Loss: 0.00723 | Val Loss: 0.00091\n",
      "Finished Epoch: 78 | Train Masked Loss: 0.00653 | Train Loss: 0.00087 | Val  Masked Loss: 0.00782 | Val Loss: 0.00106\n",
      "Finished Epoch: 79 | Train Masked Loss: 0.00639 | Train Loss: 0.00085 | Val  Masked Loss: 0.00737 | Val Loss: 0.00098\n",
      "Finished Epoch: 80 | Train Masked Loss: 0.00656 | Train Loss: 0.00087 | Val  Masked Loss: 0.00710 | Val Loss: 0.00093\n",
      "Finished Epoch: 81 | Train Masked Loss: 0.00645 | Train Loss: 0.00086 | Val  Masked Loss: 0.00714 | Val Loss: 0.00089\n",
      "Finished Epoch: 82 | Train Masked Loss: 0.00647 | Train Loss: 0.00086 | Val  Masked Loss: 0.00742 | Val Loss: 0.00099\n",
      "Finished Epoch: 83 | Train Masked Loss: 0.00642 | Train Loss: 0.00086 | Val  Masked Loss: 0.00786 | Val Loss: 0.00103\n",
      "Finished Epoch: 84 | Train Masked Loss: 0.00648 | Train Loss: 0.00086 | Val  Masked Loss: 0.00733 | Val Loss: 0.00098\n",
      "Finished Epoch: 85 | Train Masked Loss: 0.00661 | Train Loss: 0.00088 | Val  Masked Loss: 0.00839 | Val Loss: 0.00110\n",
      "Finished Epoch: 86 | Train Masked Loss: 0.00645 | Train Loss: 0.00086 | Val  Masked Loss: 0.00716 | Val Loss: 0.00089\n",
      "Finished Epoch: 87 | Train Masked Loss: 0.00635 | Train Loss: 0.00085 | Val  Masked Loss: 0.00697 | Val Loss: 0.00088\n",
      "Finished Epoch: 88 | Train Masked Loss: 0.00646 | Train Loss: 0.00086 | Val  Masked Loss: 0.00712 | Val Loss: 0.00089\n",
      "Finished Epoch: 89 | Train Masked Loss: 0.00632 | Train Loss: 0.00084 | Val  Masked Loss: 0.00721 | Val Loss: 0.00092\n",
      "Finished Epoch: 90 | Train Masked Loss: 0.00637 | Train Loss: 0.00085 | Val  Masked Loss: 0.00708 | Val Loss: 0.00091\n",
      "Finished Epoch: 91 | Train Masked Loss: 0.00629 | Train Loss: 0.00084 | Val  Masked Loss: 0.00716 | Val Loss: 0.00097\n",
      "Finished Epoch: 92 | Train Masked Loss: 0.00630 | Train Loss: 0.00084 | Val  Masked Loss: 0.00730 | Val Loss: 0.00093\n",
      "Finished Epoch: 93 | Train Masked Loss: 0.00634 | Train Loss: 0.00085 | Val  Masked Loss: 0.00722 | Val Loss: 0.00095\n",
      "Finished Epoch: 94 | Train Masked Loss: 0.00636 | Train Loss: 0.00085 | Val  Masked Loss: 0.00714 | Val Loss: 0.00090\n",
      "Finished Epoch: 95 | Train Masked Loss: 0.00638 | Train Loss: 0.00085 | Val  Masked Loss: 0.00708 | Val Loss: 0.00090\n",
      "Finished Epoch: 96 | Train Masked Loss: 0.00629 | Train Loss: 0.00084 | Val  Masked Loss: 0.00743 | Val Loss: 0.00097\n",
      "Finished Epoch: 97 | Train Masked Loss: 0.00631 | Train Loss: 0.00084 | Val  Masked Loss: 0.00743 | Val Loss: 0.00093\n",
      "Finished Epoch: 98 | Train Masked Loss: 0.00632 | Train Loss: 0.00085 | Val  Masked Loss: 0.00783 | Val Loss: 0.00097\n",
      "Finished Epoch: 99 | Train Masked Loss: 0.00666 | Train Loss: 0.00089 | Val  Masked Loss: 0.00733 | Val Loss: 0.00093\n",
      "Finished Epoch: 100 | Train Masked Loss: 0.00638 | Train Loss: 0.00085 | Val  Masked Loss: 0.00759 | Val Loss: 0.00093\n",
      "Finished Epoch: 101 | Train Masked Loss: 0.00630 | Train Loss: 0.00084 | Val  Masked Loss: 0.00728 | Val Loss: 0.00092\n",
      "Finished Epoch: 102 | Train Masked Loss: 0.00639 | Train Loss: 0.00085 | Val  Masked Loss: 0.00753 | Val Loss: 0.00099\n",
      "Finished Epoch: 103 | Train Masked Loss: 0.00628 | Train Loss: 0.00084 | Val  Masked Loss: 0.00846 | Val Loss: 0.00099\n",
      "Finished Epoch: 104 | Train Masked Loss: 0.00618 | Train Loss: 0.00082 | Val  Masked Loss: 0.00674 | Val Loss: 0.00087\n",
      "Finished Epoch: 105 | Train Masked Loss: 0.00631 | Train Loss: 0.00084 | Val  Masked Loss: 0.00676 | Val Loss: 0.00089\n",
      "Finished Epoch: 106 | Train Masked Loss: 0.00621 | Train Loss: 0.00083 | Val  Masked Loss: 0.00690 | Val Loss: 0.00089\n",
      "Finished Epoch: 107 | Train Masked Loss: 0.00626 | Train Loss: 0.00083 | Val  Masked Loss: 0.00673 | Val Loss: 0.00088\n",
      "Finished Epoch: 108 | Train Masked Loss: 0.00618 | Train Loss: 0.00082 | Val  Masked Loss: 0.00694 | Val Loss: 0.00093\n",
      "Finished Epoch: 109 | Train Masked Loss: 0.00623 | Train Loss: 0.00083 | Val  Masked Loss: 0.00702 | Val Loss: 0.00090\n",
      "Finished Epoch: 110 | Train Masked Loss: 0.00628 | Train Loss: 0.00084 | Val  Masked Loss: 0.00826 | Val Loss: 0.00109\n",
      "Finished Epoch: 111 | Train Masked Loss: 0.00623 | Train Loss: 0.00083 | Val  Masked Loss: 0.00679 | Val Loss: 0.00091\n",
      "Finished Epoch: 112 | Train Masked Loss: 0.00617 | Train Loss: 0.00082 | Val  Masked Loss: 0.00782 | Val Loss: 0.00102\n",
      "Finished Epoch: 113 | Train Masked Loss: 0.00628 | Train Loss: 0.00084 | Val  Masked Loss: 0.00678 | Val Loss: 0.00089\n",
      "Finished Epoch: 114 | Train Masked Loss: 0.00615 | Train Loss: 0.00082 | Val  Masked Loss: 0.00681 | Val Loss: 0.00088\n",
      "Finished Epoch: 115 | Train Masked Loss: 0.00620 | Train Loss: 0.00083 | Val  Masked Loss: 0.00924 | Val Loss: 0.00120\n",
      "Finished Epoch: 116 | Train Masked Loss: 0.00627 | Train Loss: 0.00084 | Val  Masked Loss: 0.00705 | Val Loss: 0.00089\n",
      "Finished Epoch: 117 | Train Masked Loss: 0.00621 | Train Loss: 0.00083 | Val  Masked Loss: 0.00725 | Val Loss: 0.00094\n",
      "Finished Epoch: 118 | Train Masked Loss: 0.00620 | Train Loss: 0.00083 | Val  Masked Loss: 0.00709 | Val Loss: 0.00089\n",
      "Finished Epoch: 119 | Train Masked Loss: 0.00613 | Train Loss: 0.00082 | Val  Masked Loss: 0.00691 | Val Loss: 0.00092\n",
      "Finished Epoch: 120 | Train Masked Loss: 0.00612 | Train Loss: 0.00082 | Val  Masked Loss: 0.00704 | Val Loss: 0.00092\n",
      "Finished Epoch: 121 | Train Masked Loss: 0.00615 | Train Loss: 0.00082 | Val  Masked Loss: 0.00889 | Val Loss: 0.00118\n",
      "Finished Epoch: 122 | Train Masked Loss: 0.00626 | Train Loss: 0.00083 | Val  Masked Loss: 0.00796 | Val Loss: 0.00105\n",
      "Finished Epoch: 123 | Train Masked Loss: 0.00615 | Train Loss: 0.00082 | Val  Masked Loss: 0.00707 | Val Loss: 0.00090\n",
      "Finished Epoch: 124 | Train Masked Loss: 0.00618 | Train Loss: 0.00083 | Val  Masked Loss: 0.00729 | Val Loss: 0.00090\n",
      "Finished Epoch: 125 | Train Masked Loss: 0.00610 | Train Loss: 0.00081 | Val  Masked Loss: 0.00676 | Val Loss: 0.00087\n",
      "Finished Epoch: 126 | Train Masked Loss: 0.00642 | Train Loss: 0.00085 | Val  Masked Loss: 0.00775 | Val Loss: 0.00095\n",
      "Finished Epoch: 127 | Train Masked Loss: 0.00618 | Train Loss: 0.00082 | Val  Masked Loss: 0.00778 | Val Loss: 0.00103\n",
      "Finished Epoch: 128 | Train Masked Loss: 0.00614 | Train Loss: 0.00082 | Val  Masked Loss: 0.00724 | Val Loss: 0.00092\n",
      "Finished Epoch: 129 | Train Masked Loss: 0.00667 | Train Loss: 0.00089 | Val  Masked Loss: 0.00749 | Val Loss: 0.00100\n",
      "Finished Epoch: 130 | Train Masked Loss: 0.00624 | Train Loss: 0.00084 | Val  Masked Loss: 0.00696 | Val Loss: 0.00091\n",
      "Finished Epoch: 131 | Train Masked Loss: 0.00619 | Train Loss: 0.00083 | Val  Masked Loss: 0.00734 | Val Loss: 0.00097\n",
      "Finished Epoch: 132 | Train Masked Loss: 0.00623 | Train Loss: 0.00083 | Val  Masked Loss: 0.00719 | Val Loss: 0.00095\n",
      "Finished Epoch: 133 | Train Masked Loss: 0.00617 | Train Loss: 0.00082 | Val  Masked Loss: 0.00736 | Val Loss: 0.00095\n",
      "Finished Epoch: 134 | Train Masked Loss: 0.00618 | Train Loss: 0.00082 | Val  Masked Loss: 0.00707 | Val Loss: 0.00095\n",
      "Finished Epoch: 135 | Train Masked Loss: 0.00612 | Train Loss: 0.00082 | Val  Masked Loss: 0.00728 | Val Loss: 0.00092\n",
      "Finished Epoch: 136 | Train Masked Loss: 0.00618 | Train Loss: 0.00082 | Val  Masked Loss: 0.00875 | Val Loss: 0.00116\n",
      "Finished Epoch: 137 | Train Masked Loss: 0.00615 | Train Loss: 0.00082 | Val  Masked Loss: 0.00712 | Val Loss: 0.00089\n",
      "Finished Epoch: 138 | Train Masked Loss: 0.00618 | Train Loss: 0.00082 | Val  Masked Loss: 0.00715 | Val Loss: 0.00092\n",
      "Finished Epoch: 139 | Train Masked Loss: 0.00616 | Train Loss: 0.00082 | Val  Masked Loss: 0.00702 | Val Loss: 0.00088\n",
      "Finished Epoch: 140 | Train Masked Loss: 0.00616 | Train Loss: 0.00082 | Val  Masked Loss: 0.00732 | Val Loss: 0.00095\n",
      "Finished Epoch: 141 | Train Masked Loss: 0.00612 | Train Loss: 0.00082 | Val  Masked Loss: 0.00729 | Val Loss: 0.00093\n",
      "Finished Epoch: 142 | Train Masked Loss: 0.00614 | Train Loss: 0.00082 | Val  Masked Loss: 0.00817 | Val Loss: 0.00110\n",
      "Finished Epoch: 143 | Train Masked Loss: 0.00618 | Train Loss: 0.00082 | Val  Masked Loss: 0.00729 | Val Loss: 0.00097\n",
      "Finished Epoch: 144 | Train Masked Loss: 0.00611 | Train Loss: 0.00081 | Val  Masked Loss: 0.00719 | Val Loss: 0.00091\n",
      "Finished Epoch: 145 | Train Masked Loss: 0.00608 | Train Loss: 0.00081 | Val  Masked Loss: 0.00687 | Val Loss: 0.00088\n",
      "Finished Epoch: 146 | Train Masked Loss: 0.00609 | Train Loss: 0.00081 | Val  Masked Loss: 0.00686 | Val Loss: 0.00090\n",
      "Finished Epoch: 147 | Train Masked Loss: 0.00616 | Train Loss: 0.00082 | Val  Masked Loss: 0.00699 | Val Loss: 0.00091\n",
      "Finished Epoch: 148 | Train Masked Loss: 0.00609 | Train Loss: 0.00081 | Val  Masked Loss: 0.00735 | Val Loss: 0.00093\n",
      "Finished Epoch: 149 | Train Masked Loss: 0.00607 | Train Loss: 0.00081 | Val  Masked Loss: 0.00725 | Val Loss: 0.00094\n",
      "Finished Epoch: 150 | Train Masked Loss: 0.00602 | Train Loss: 0.00080 | Val  Masked Loss: 0.00772 | Val Loss: 0.00094\n",
      "Finished Epoch: 151 | Train Masked Loss: 0.00615 | Train Loss: 0.00082 | Val  Masked Loss: 0.00703 | Val Loss: 0.00091\n",
      "Finished Epoch: 152 | Train Masked Loss: 0.00609 | Train Loss: 0.00081 | Val  Masked Loss: 0.00747 | Val Loss: 0.00096\n",
      "Finished Epoch: 153 | Train Masked Loss: 0.00611 | Train Loss: 0.00082 | Val  Masked Loss: 0.00768 | Val Loss: 0.00100\n",
      "Finished Epoch: 154 | Train Masked Loss: 0.00614 | Train Loss: 0.00082 | Val  Masked Loss: 0.00972 | Val Loss: 0.00129\n",
      "Finished Epoch: 155 | Train Masked Loss: 0.00604 | Train Loss: 0.00080 | Val  Masked Loss: 0.00734 | Val Loss: 0.00094\n",
      "Finished Epoch: 156 | Train Masked Loss: 0.00604 | Train Loss: 0.00080 | Val  Masked Loss: 0.00746 | Val Loss: 0.00098\n",
      "Finished Epoch: 157 | Train Masked Loss: 0.00605 | Train Loss: 0.00080 | Val  Masked Loss: 0.00731 | Val Loss: 0.00096\n",
      "Finished Epoch: 158 | Train Masked Loss: 0.00615 | Train Loss: 0.00082 | Val  Masked Loss: 0.00724 | Val Loss: 0.00091\n",
      "Finished Epoch: 159 | Train Masked Loss: 0.00603 | Train Loss: 0.00080 | Val  Masked Loss: 0.00698 | Val Loss: 0.00091\n",
      "Finished Epoch: 160 | Train Masked Loss: 0.00606 | Train Loss: 0.00080 | Val  Masked Loss: 0.00792 | Val Loss: 0.00105\n",
      "Finished Epoch: 161 | Train Masked Loss: 0.00602 | Train Loss: 0.00080 | Val  Masked Loss: 0.00689 | Val Loss: 0.00089\n",
      "Finished Epoch: 162 | Train Masked Loss: 0.00603 | Train Loss: 0.00080 | Val  Masked Loss: 0.00720 | Val Loss: 0.00095\n",
      "Finished Epoch: 163 | Train Masked Loss: 0.00599 | Train Loss: 0.00080 | Val  Masked Loss: 0.00711 | Val Loss: 0.00094\n",
      "Finished Epoch: 164 | Train Masked Loss: 0.00602 | Train Loss: 0.00080 | Val  Masked Loss: 0.00707 | Val Loss: 0.00090\n",
      "Finished Epoch: 165 | Train Masked Loss: 0.00610 | Train Loss: 0.00081 | Val  Masked Loss: 0.00732 | Val Loss: 0.00092\n",
      "Finished Epoch: 166 | Train Masked Loss: 0.00609 | Train Loss: 0.00081 | Val  Masked Loss: 0.00695 | Val Loss: 0.00093\n",
      "Finished Epoch: 167 | Train Masked Loss: 0.00589 | Train Loss: 0.00079 | Val  Masked Loss: 0.00874 | Val Loss: 0.00117\n",
      "Finished Epoch: 168 | Train Masked Loss: 0.00599 | Train Loss: 0.00080 | Val  Masked Loss: 0.00793 | Val Loss: 0.00104\n",
      "Finished Epoch: 169 | Train Masked Loss: 0.00596 | Train Loss: 0.00080 | Val  Masked Loss: 0.00724 | Val Loss: 0.00091\n",
      "Finished Epoch: 170 | Train Masked Loss: 0.00610 | Train Loss: 0.00081 | Val  Masked Loss: 0.00854 | Val Loss: 0.00100\n",
      "Finished Epoch: 171 | Train Masked Loss: 0.00606 | Train Loss: 0.00080 | Val  Masked Loss: 0.00771 | Val Loss: 0.00103\n",
      "Finished Epoch: 172 | Train Masked Loss: 0.00598 | Train Loss: 0.00079 | Val  Masked Loss: 0.00713 | Val Loss: 0.00091\n",
      "Finished Epoch: 173 | Train Masked Loss: 0.00597 | Train Loss: 0.00079 | Val  Masked Loss: 0.00681 | Val Loss: 0.00088\n",
      "Finished Epoch: 174 | Train Masked Loss: 0.00601 | Train Loss: 0.00080 | Val  Masked Loss: 0.00701 | Val Loss: 0.00088\n",
      "Finished Epoch: 175 | Train Masked Loss: 0.00594 | Train Loss: 0.00079 | Val  Masked Loss: 0.00730 | Val Loss: 0.00097\n",
      "Finished Epoch: 176 | Train Masked Loss: 0.00594 | Train Loss: 0.00079 | Val  Masked Loss: 0.00863 | Val Loss: 0.00100\n",
      "Finished Epoch: 177 | Train Masked Loss: 0.00603 | Train Loss: 0.00080 | Val  Masked Loss: 0.00675 | Val Loss: 0.00087\n",
      "Finished Epoch: 178 | Train Masked Loss: 0.00611 | Train Loss: 0.00081 | Val  Masked Loss: 0.00747 | Val Loss: 0.00092\n",
      "Finished Epoch: 179 | Train Masked Loss: 0.00605 | Train Loss: 0.00080 | Val  Masked Loss: 0.00750 | Val Loss: 0.00096\n",
      "Finished Epoch: 180 | Train Masked Loss: 0.00599 | Train Loss: 0.00079 | Val  Masked Loss: 0.00749 | Val Loss: 0.00091\n",
      "Finished Epoch: 181 | Train Masked Loss: 0.00599 | Train Loss: 0.00080 | Val  Masked Loss: 0.00715 | Val Loss: 0.00090\n",
      "Finished Epoch: 182 | Train Masked Loss: 0.00600 | Train Loss: 0.00080 | Val  Masked Loss: 0.00739 | Val Loss: 0.00090\n",
      "Finished Epoch: 183 | Train Masked Loss: 0.00595 | Train Loss: 0.00079 | Val  Masked Loss: 0.00742 | Val Loss: 0.00095\n",
      "Finished Epoch: 184 | Train Masked Loss: 0.00605 | Train Loss: 0.00080 | Val  Masked Loss: 0.00719 | Val Loss: 0.00090\n",
      "Finished Epoch: 185 | Train Masked Loss: 0.00591 | Train Loss: 0.00079 | Val  Masked Loss: 0.00747 | Val Loss: 0.00098\n",
      "Finished Epoch: 186 | Train Masked Loss: 0.00591 | Train Loss: 0.00078 | Val  Masked Loss: 0.00714 | Val Loss: 0.00093\n",
      "Finished Epoch: 187 | Train Masked Loss: 0.00591 | Train Loss: 0.00079 | Val  Masked Loss: 0.00796 | Val Loss: 0.00106\n",
      "Finished Epoch: 188 | Train Masked Loss: 0.00597 | Train Loss: 0.00080 | Val  Masked Loss: 0.00835 | Val Loss: 0.00097\n",
      "Finished Epoch: 189 | Train Masked Loss: 0.00601 | Train Loss: 0.00080 | Val  Masked Loss: 0.00753 | Val Loss: 0.00092\n",
      "Finished Epoch: 190 | Train Masked Loss: 0.00593 | Train Loss: 0.00079 | Val  Masked Loss: 0.00801 | Val Loss: 0.00106\n",
      "Finished Epoch: 191 | Train Masked Loss: 0.00599 | Train Loss: 0.00079 | Val  Masked Loss: 0.00687 | Val Loss: 0.00087\n",
      "Finished Epoch: 192 | Train Masked Loss: 0.00592 | Train Loss: 0.00079 | Val  Masked Loss: 0.00664 | Val Loss: 0.00088\n",
      "Finished Epoch: 193 | Train Masked Loss: 0.00592 | Train Loss: 0.00079 | Val  Masked Loss: 0.00740 | Val Loss: 0.00098\n",
      "Finished Epoch: 194 | Train Masked Loss: 0.00602 | Train Loss: 0.00079 | Val  Masked Loss: 0.00686 | Val Loss: 0.00089\n",
      "Finished Epoch: 195 | Train Masked Loss: 0.00600 | Train Loss: 0.00079 | Val  Masked Loss: 0.00700 | Val Loss: 0.00089\n",
      "Finished Epoch: 196 | Train Masked Loss: 0.00601 | Train Loss: 0.00079 | Val  Masked Loss: 0.00889 | Val Loss: 0.00121\n",
      "Finished Epoch: 197 | Train Masked Loss: 0.00594 | Train Loss: 0.00079 | Val  Masked Loss: 0.00758 | Val Loss: 0.00100\n",
      "Finished Epoch: 198 | Train Masked Loss: 0.00599 | Train Loss: 0.00079 | Val  Masked Loss: 0.00774 | Val Loss: 0.00102\n",
      "Finished Epoch: 199 | Train Masked Loss: 0.00594 | Train Loss: 0.00079 | Val  Masked Loss: 0.00698 | Val Loss: 0.00092\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# model = ResNet()\n",
    "\n",
    "model = U_Net()\n",
    "\n",
    "epochs = 200\n",
    "learning_rate = 2e-4\n",
    "scheduler = None\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, verbose=True)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.0005)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "trainer = Trainer(model, optimizer, loss_fn, train_dataloader, valid_dataloader, scheduler=scheduler)\n",
    "trained_model = trainer.run(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))\n",
    "trained_model=model\n",
    "trained_model.eval()\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    data = iter(train_dataloader).next()\n",
    "    image, label = data['img'].to(device, dtype=torch.float), data['thick_map'].to(device, dtype=torch.float)\n",
    "    output = trained_model(image)\n",
    "    image = image.cpu().numpy().transpose(0,2,3,1)\n",
    "    label = label.cpu().numpy()\n",
    "    output = output.cpu().numpy()\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.subplot(1,4,1)\n",
    "    plt.imshow(image[0], cmap='gray')\n",
    "    plt.title('Input Image')\n",
    "    plt.subplot(1,4,2)\n",
    "    plt.imshow(label[0].reshape(128,128), cmap='gray')\n",
    "    plt.title('Ground Truth')\n",
    "    plt.subplot(1,4,3)\n",
    "    plt.imshow(output[0].reshape(128,128), cmap='gray')\n",
    "    plt.title('Predicted Thickness Map')\n",
    "    plt.subplot(1,4,4)\n",
    "    plt.imshow(abs(label[0]-output[0]).reshape(128,128), cmap='gray')\n",
    "    plt.title('Diff')\n",
    "    plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projectv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "270a635b83e65d335eba5d097089b0ff08369f0634f543893019998ee28e2990"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
